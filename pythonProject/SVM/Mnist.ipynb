{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOop+fwU+38Z7SSy7sPAwNF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/datle2403/datle2403/blob/master/pythonProject/SVM/Mnist.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "Oj_aZinq1-t_",
        "outputId": "81ab7c69-c658-41d3-bcbe-2b95ec1ec7ec"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 7200x7200 with 0 Axes>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import fetch_openml\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "f = plt.figure()\n",
        "f.set_figwidth(100)\n",
        "f.set_figheight(100)\n",
        "\n",
        "def standardized(dataset):\n",
        "    scaler = StandardScaler()\n",
        "    scaler.fit(dataset)\n",
        "    dataset = scaler.transform(dataset)\n",
        "    return dataset\n",
        "def create_dataset():\n",
        "    df=pd.read_csv('mnist_784_csv.csv')\n",
        "    df.dropna(inplace=True)\n",
        "    return df\n",
        "def split(df):\n",
        "  y=df['class'].to_numpy()\n",
        "  X=df.drop('class',axis=1).to_numpy()\n",
        "  return X,y\n",
        "def pca(data,num):\n",
        "    pca = PCA(n_components=num) # 95% of variance equal to 330 Principal component\n",
        "    pca.fit(data)\n",
        "    data = pca.transform(data)\n",
        "    print(np.sum(pca.explained_variance_ratio_) )\n",
        "    return data"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df=create_dataset()\n",
        "X,y=split(df)\n",
        "X.shape # 70K ex, 784 feature\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x2pyv0zN3IQh",
        "outputId": "ab9d1be4-090d-4662-d629-7d6a8fca6877"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(70000, 784)"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RXd_Vs9xAjJ7",
        "outputId": "81d18e79-d2f7-4185-c13a-ad4fb38c9ab2"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([5, 0, 4, ..., 4, 5, 6])"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "X_pca=pca(X,100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c85n4-mD3KaX",
        "outputId": "3ca218b2-07cd-424a-e673-d3271d963cf2"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9146638863541573\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_pca.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0q1f-8fd6Oer",
        "outputId": "1912fb0f-9798-49e7-d3ad-ee44db28a0f7"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(70000, 100)"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#split test_train set\n",
        "def split_train_test(X,y):\n",
        "  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
        "  return X_train, X_test, y_train, y_test\n",
        "def convert_to_fit(X,y):\n",
        "  feature=X.shape[1]\n",
        "  W=np.random.randn(feature+1, 10)*0.001 # 331 feature, with 1 feature is b\n",
        "  one=np.empty((X.shape[0],1))\n",
        "  one.fill(1)\n",
        "  X_pc=np.hstack((X,one))\n",
        "  X_pc=X_pc.T\n",
        "  return X_pc,y,W"
      ],
      "metadata": {
        "id": "o_IElg2yM7gX"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def svm_loss_vectorized(W, X, y, reg):\n",
        "    d, C = W.shape \n",
        "    _, N = X.shape \n",
        "    loss = 0 \n",
        "    dW = np.zeros_like(W)\n",
        "    \n",
        "    Z = W.T.dot(X)     \n",
        "    \n",
        "    correct_class_score = np.choose(y, Z).reshape(N,1).T\n",
        "    margins = np.maximum(0, Z - correct_class_score + 1) \n",
        "    margins[y, np.arange(margins.shape[1])] = 0.\n",
        "    loss = np.sum(margins, axis = (0, 1))\n",
        "    loss /= N \n",
        "    loss += 0.5 * reg * np.sum(W * W)\n",
        "    \n",
        "    F = (margins > 0).astype(int)\n",
        "    F[y, np.arange(F.shape[1])] = np.sum(-F, axis = 0)\n",
        "    dW = X.dot(F.T)/N + reg*W\n",
        "    return loss, dW\n",
        "def multiclass_svm_GD(X, y, Winit, reg, lr=.1, \\\n",
        "        batch_size = 100, num_iters = 1000, print_every = 100):\n",
        "    W = Winit \n",
        "    loss_history = np.zeros((num_iters))\n",
        "    for it in range(num_iters):\n",
        "        # randomly pick a batch of X\n",
        "        idx = np.random.choice(X.shape[1], batch_size)\n",
        "        X_batch = X[:, idx]\n",
        "        y_batch = y[idx]\n",
        "\n",
        "        loss_history[it], dW = \\\n",
        "            svm_loss_vectorized(W, X_batch, y_batch, reg)\n",
        "\n",
        "        W -= lr*dW \n",
        "        if it % print_every == 1:\n",
        "            print ('it %d/%d, loss = %f' \\\n",
        "                %(it, num_iters, loss_history[it]))\n",
        "\n",
        "    return W, loss_history \n",
        "def run(X,y,Winit,iter,reg,lr):\n",
        "  W, loss_history =multiclass_svm_GD(X, y, Winit, reg, lr=lr, \\\n",
        "        batch_size = 500, num_iters =iter, print_every = 10)\n",
        "  return W\n",
        "def predict(X,y,W):\n",
        "  N=X.shape[1]\n",
        "  Z = W.T.dot(X)\n",
        "  Z=Z.T\n",
        "  r=[]\n",
        "  for x in Z:\n",
        "    r.append(np.argmax(x,axis=0))\n",
        "  count=0\n",
        "  for m in range(0,N):\n",
        "    if r[m]==y[m]:\n",
        "      count+=1\n",
        "  score=(count/N)\n",
        "  return score"
      ],
      "metadata": {
        "id": "SJSUlpRu3Xze"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test=split_train_test(X_pca,y)\n",
        "print(X_train.shape)\n",
        "X_train,y_train,W_train=convert_to_fit(X_train,y_train)\n",
        "X_test,y_test,W_test=convert_to_fit(X_test,y_test)\n",
        "W1=run(X_train,y_train,W_train,5000,0.1,0.00001)\n",
        "print('score for train:',predict(X_train,y_train,W1))\n",
        "print('score for test:',predict(X_test,y_test,W1))\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XqqKs27XPvnh",
        "outputId": "ed568f6f-d65a-4e88-c4ec-6f5a34ed5ffe"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(46900, 100)\n",
            "it 1/5000, loss = 2.542935\n",
            "it 11/5000, loss = 1.077483\n",
            "it 21/5000, loss = 0.835389\n",
            "it 31/5000, loss = 0.635999\n",
            "it 41/5000, loss = 0.599394\n",
            "it 51/5000, loss = 0.596737\n",
            "it 61/5000, loss = 0.574257\n",
            "it 71/5000, loss = 0.403892\n",
            "it 81/5000, loss = 0.803564\n",
            "it 91/5000, loss = 0.508283\n",
            "it 101/5000, loss = 0.578158\n",
            "it 111/5000, loss = 0.510758\n",
            "it 121/5000, loss = 0.558411\n",
            "it 131/5000, loss = 0.445244\n",
            "it 141/5000, loss = 0.474949\n",
            "it 151/5000, loss = 0.463721\n",
            "it 161/5000, loss = 0.398252\n",
            "it 171/5000, loss = 0.452888\n",
            "it 181/5000, loss = 0.397055\n",
            "it 191/5000, loss = 0.516117\n",
            "it 201/5000, loss = 0.426090\n",
            "it 211/5000, loss = 0.379507\n",
            "it 221/5000, loss = 0.402915\n",
            "it 231/5000, loss = 0.442085\n",
            "it 241/5000, loss = 0.387076\n",
            "it 251/5000, loss = 0.473510\n",
            "it 261/5000, loss = 0.585422\n",
            "it 271/5000, loss = 0.343443\n",
            "it 281/5000, loss = 0.517413\n",
            "it 291/5000, loss = 0.659529\n",
            "it 301/5000, loss = 0.421912\n",
            "it 311/5000, loss = 0.407888\n",
            "it 321/5000, loss = 0.515815\n",
            "it 331/5000, loss = 0.454088\n",
            "it 341/5000, loss = 0.565459\n",
            "it 351/5000, loss = 0.527908\n",
            "it 361/5000, loss = 0.488768\n",
            "it 371/5000, loss = 0.299611\n",
            "it 381/5000, loss = 0.355968\n",
            "it 391/5000, loss = 0.383417\n",
            "it 401/5000, loss = 0.529812\n",
            "it 411/5000, loss = 0.406867\n",
            "it 421/5000, loss = 0.366936\n",
            "it 431/5000, loss = 0.418215\n",
            "it 441/5000, loss = 0.466461\n",
            "it 451/5000, loss = 0.511908\n",
            "it 461/5000, loss = 0.436013\n",
            "it 471/5000, loss = 0.483310\n",
            "it 481/5000, loss = 0.395899\n",
            "it 491/5000, loss = 0.553767\n",
            "it 501/5000, loss = 0.421273\n",
            "it 511/5000, loss = 0.581033\n",
            "it 521/5000, loss = 0.550257\n",
            "it 531/5000, loss = 0.364969\n",
            "it 541/5000, loss = 0.547286\n",
            "it 551/5000, loss = 0.368614\n",
            "it 561/5000, loss = 0.457435\n",
            "it 571/5000, loss = 0.675086\n",
            "it 581/5000, loss = 0.549673\n",
            "it 591/5000, loss = 0.462153\n",
            "it 601/5000, loss = 0.344211\n",
            "it 611/5000, loss = 0.405681\n",
            "it 621/5000, loss = 0.474067\n",
            "it 631/5000, loss = 0.370804\n",
            "it 641/5000, loss = 0.692538\n",
            "it 651/5000, loss = 0.391383\n",
            "it 661/5000, loss = 0.428897\n",
            "it 671/5000, loss = 0.473330\n",
            "it 681/5000, loss = 0.533637\n",
            "it 691/5000, loss = 0.439075\n",
            "it 701/5000, loss = 0.551633\n",
            "it 711/5000, loss = 0.585480\n",
            "it 721/5000, loss = 0.549359\n",
            "it 731/5000, loss = 0.382291\n",
            "it 741/5000, loss = 0.396178\n",
            "it 751/5000, loss = 0.500129\n",
            "it 761/5000, loss = 0.588567\n",
            "it 771/5000, loss = 0.426361\n",
            "it 781/5000, loss = 0.467716\n",
            "it 791/5000, loss = 0.539379\n",
            "it 801/5000, loss = 0.564605\n",
            "it 811/5000, loss = 0.596593\n",
            "it 821/5000, loss = 0.503208\n",
            "it 831/5000, loss = 0.350897\n",
            "it 841/5000, loss = 0.502101\n",
            "it 851/5000, loss = 0.356701\n",
            "it 861/5000, loss = 0.526139\n",
            "it 871/5000, loss = 0.418233\n",
            "it 881/5000, loss = 0.496177\n",
            "it 891/5000, loss = 0.424175\n",
            "it 901/5000, loss = 0.384091\n",
            "it 911/5000, loss = 0.378654\n",
            "it 921/5000, loss = 0.365474\n",
            "it 931/5000, loss = 0.372735\n",
            "it 941/5000, loss = 0.505029\n",
            "it 951/5000, loss = 0.396814\n",
            "it 961/5000, loss = 0.409938\n",
            "it 971/5000, loss = 0.569057\n",
            "it 981/5000, loss = 0.462383\n",
            "it 991/5000, loss = 0.506399\n",
            "it 1001/5000, loss = 0.411883\n",
            "it 1011/5000, loss = 0.493367\n",
            "it 1021/5000, loss = 0.363800\n",
            "it 1031/5000, loss = 0.424857\n",
            "it 1041/5000, loss = 0.648979\n",
            "it 1051/5000, loss = 0.346636\n",
            "it 1061/5000, loss = 0.399695\n",
            "it 1071/5000, loss = 0.440468\n",
            "it 1081/5000, loss = 0.374525\n",
            "it 1091/5000, loss = 0.467914\n",
            "it 1101/5000, loss = 0.448559\n",
            "it 1111/5000, loss = 0.428243\n",
            "it 1121/5000, loss = 0.356650\n",
            "it 1131/5000, loss = 0.450683\n",
            "it 1141/5000, loss = 0.399825\n",
            "it 1151/5000, loss = 0.410640\n",
            "it 1161/5000, loss = 0.513360\n",
            "it 1171/5000, loss = 0.520811\n",
            "it 1181/5000, loss = 0.376700\n",
            "it 1191/5000, loss = 0.338374\n",
            "it 1201/5000, loss = 0.425138\n",
            "it 1211/5000, loss = 0.689168\n",
            "it 1221/5000, loss = 0.529978\n",
            "it 1231/5000, loss = 0.486833\n",
            "it 1241/5000, loss = 0.389294\n",
            "it 1251/5000, loss = 0.509852\n",
            "it 1261/5000, loss = 0.544469\n",
            "it 1271/5000, loss = 0.388721\n",
            "it 1281/5000, loss = 0.505340\n",
            "it 1291/5000, loss = 0.464993\n",
            "it 1301/5000, loss = 0.527623\n",
            "it 1311/5000, loss = 0.384685\n",
            "it 1321/5000, loss = 0.254781\n",
            "it 1331/5000, loss = 0.357294\n",
            "it 1341/5000, loss = 0.486450\n",
            "it 1351/5000, loss = 0.449831\n",
            "it 1361/5000, loss = 0.680748\n",
            "it 1371/5000, loss = 0.385841\n",
            "it 1381/5000, loss = 0.453009\n",
            "it 1391/5000, loss = 0.436270\n",
            "it 1401/5000, loss = 0.524915\n",
            "it 1411/5000, loss = 0.416992\n",
            "it 1421/5000, loss = 0.307628\n",
            "it 1431/5000, loss = 0.441890\n",
            "it 1441/5000, loss = 0.502080\n",
            "it 1451/5000, loss = 0.396285\n",
            "it 1461/5000, loss = 0.387867\n",
            "it 1471/5000, loss = 0.463206\n",
            "it 1481/5000, loss = 0.424649\n",
            "it 1491/5000, loss = 0.588739\n",
            "it 1501/5000, loss = 0.451739\n",
            "it 1511/5000, loss = 0.382163\n",
            "it 1521/5000, loss = 0.315402\n",
            "it 1531/5000, loss = 0.296552\n",
            "it 1541/5000, loss = 0.289630\n",
            "it 1551/5000, loss = 0.407047\n",
            "it 1561/5000, loss = 0.351562\n",
            "it 1571/5000, loss = 0.452746\n",
            "it 1581/5000, loss = 0.382691\n",
            "it 1591/5000, loss = 0.354244\n",
            "it 1601/5000, loss = 0.490750\n",
            "it 1611/5000, loss = 0.433832\n",
            "it 1621/5000, loss = 0.569519\n",
            "it 1631/5000, loss = 0.491958\n",
            "it 1641/5000, loss = 0.478932\n",
            "it 1651/5000, loss = 0.625709\n",
            "it 1661/5000, loss = 0.393827\n",
            "it 1671/5000, loss = 0.446990\n",
            "it 1681/5000, loss = 0.317671\n",
            "it 1691/5000, loss = 0.615064\n",
            "it 1701/5000, loss = 0.460822\n",
            "it 1711/5000, loss = 0.356200\n",
            "it 1721/5000, loss = 0.349886\n",
            "it 1731/5000, loss = 0.435053\n",
            "it 1741/5000, loss = 0.659597\n",
            "it 1751/5000, loss = 0.385917\n",
            "it 1761/5000, loss = 0.467244\n",
            "it 1771/5000, loss = 0.468317\n",
            "it 1781/5000, loss = 0.409079\n",
            "it 1791/5000, loss = 0.365865\n",
            "it 1801/5000, loss = 0.450355\n",
            "it 1811/5000, loss = 0.455570\n",
            "it 1821/5000, loss = 0.493602\n",
            "it 1831/5000, loss = 0.422288\n",
            "it 1841/5000, loss = 0.350082\n",
            "it 1851/5000, loss = 0.304023\n",
            "it 1861/5000, loss = 0.393933\n",
            "it 1871/5000, loss = 0.410580\n",
            "it 1881/5000, loss = 0.490568\n",
            "it 1891/5000, loss = 0.519156\n",
            "it 1901/5000, loss = 0.474459\n",
            "it 1911/5000, loss = 0.394962\n",
            "it 1921/5000, loss = 0.329618\n",
            "it 1931/5000, loss = 0.449075\n",
            "it 1941/5000, loss = 0.343084\n",
            "it 1951/5000, loss = 0.520272\n",
            "it 1961/5000, loss = 0.452792\n",
            "it 1971/5000, loss = 0.373008\n",
            "it 1981/5000, loss = 0.529307\n",
            "it 1991/5000, loss = 0.429956\n",
            "it 2001/5000, loss = 0.438029\n",
            "it 2011/5000, loss = 0.375402\n",
            "it 2021/5000, loss = 0.350483\n",
            "it 2031/5000, loss = 0.340147\n",
            "it 2041/5000, loss = 0.372554\n",
            "it 2051/5000, loss = 0.425789\n",
            "it 2061/5000, loss = 0.489629\n",
            "it 2071/5000, loss = 0.505568\n",
            "it 2081/5000, loss = 0.474360\n",
            "it 2091/5000, loss = 0.425467\n",
            "it 2101/5000, loss = 0.377761\n",
            "it 2111/5000, loss = 0.522415\n",
            "it 2121/5000, loss = 0.391156\n",
            "it 2131/5000, loss = 0.588689\n",
            "it 2141/5000, loss = 0.491562\n",
            "it 2151/5000, loss = 0.449952\n",
            "it 2161/5000, loss = 0.455073\n",
            "it 2171/5000, loss = 0.526931\n",
            "it 2181/5000, loss = 0.275342\n",
            "it 2191/5000, loss = 0.574718\n",
            "it 2201/5000, loss = 0.311944\n",
            "it 2211/5000, loss = 0.372495\n",
            "it 2221/5000, loss = 0.565450\n",
            "it 2231/5000, loss = 0.428994\n",
            "it 2241/5000, loss = 0.293200\n",
            "it 2251/5000, loss = 0.532979\n",
            "it 2261/5000, loss = 0.382551\n",
            "it 2271/5000, loss = 0.457979\n",
            "it 2281/5000, loss = 0.433007\n",
            "it 2291/5000, loss = 0.634318\n",
            "it 2301/5000, loss = 0.463517\n",
            "it 2311/5000, loss = 0.530533\n",
            "it 2321/5000, loss = 0.594830\n",
            "it 2331/5000, loss = 0.530259\n",
            "it 2341/5000, loss = 0.389139\n",
            "it 2351/5000, loss = 0.418710\n",
            "it 2361/5000, loss = 0.369095\n",
            "it 2371/5000, loss = 0.426361\n",
            "it 2381/5000, loss = 0.525624\n",
            "it 2391/5000, loss = 0.480784\n",
            "it 2401/5000, loss = 0.372709\n",
            "it 2411/5000, loss = 0.378132\n",
            "it 2421/5000, loss = 0.402760\n",
            "it 2431/5000, loss = 0.424912\n",
            "it 2441/5000, loss = 0.568531\n",
            "it 2451/5000, loss = 0.697448\n",
            "it 2461/5000, loss = 0.451599\n",
            "it 2471/5000, loss = 0.512085\n",
            "it 2481/5000, loss = 0.444100\n",
            "it 2491/5000, loss = 0.460295\n",
            "it 2501/5000, loss = 0.289718\n",
            "it 2511/5000, loss = 0.615827\n",
            "it 2521/5000, loss = 0.533923\n",
            "it 2531/5000, loss = 0.451037\n",
            "it 2541/5000, loss = 0.545291\n",
            "it 2551/5000, loss = 0.437918\n",
            "it 2561/5000, loss = 0.470759\n",
            "it 2571/5000, loss = 0.592532\n",
            "it 2581/5000, loss = 0.498986\n",
            "it 2591/5000, loss = 0.604333\n",
            "it 2601/5000, loss = 0.558604\n",
            "it 2611/5000, loss = 0.300407\n",
            "it 2621/5000, loss = 0.386016\n",
            "it 2631/5000, loss = 0.535483\n",
            "it 2641/5000, loss = 0.308471\n",
            "it 2651/5000, loss = 0.378723\n",
            "it 2661/5000, loss = 0.444712\n",
            "it 2671/5000, loss = 0.343649\n",
            "it 2681/5000, loss = 0.615318\n",
            "it 2691/5000, loss = 0.586350\n",
            "it 2701/5000, loss = 0.383042\n",
            "it 2711/5000, loss = 0.363103\n",
            "it 2721/5000, loss = 0.366360\n",
            "it 2731/5000, loss = 0.398447\n",
            "it 2741/5000, loss = 0.504372\n",
            "it 2751/5000, loss = 0.572876\n",
            "it 2761/5000, loss = 0.359421\n",
            "it 2771/5000, loss = 0.494166\n",
            "it 2781/5000, loss = 0.393980\n",
            "it 2791/5000, loss = 0.359867\n",
            "it 2801/5000, loss = 0.357217\n",
            "it 2811/5000, loss = 0.437234\n",
            "it 2821/5000, loss = 0.413125\n",
            "it 2831/5000, loss = 0.460783\n",
            "it 2841/5000, loss = 0.400115\n",
            "it 2851/5000, loss = 0.528149\n",
            "it 2861/5000, loss = 0.449751\n",
            "it 2871/5000, loss = 0.525215\n",
            "it 2881/5000, loss = 0.460235\n",
            "it 2891/5000, loss = 0.553037\n",
            "it 2901/5000, loss = 0.532984\n",
            "it 2911/5000, loss = 0.293446\n",
            "it 2921/5000, loss = 0.343537\n",
            "it 2931/5000, loss = 0.415413\n",
            "it 2941/5000, loss = 0.448689\n",
            "it 2951/5000, loss = 0.365183\n",
            "it 2961/5000, loss = 0.459311\n",
            "it 2971/5000, loss = 0.327652\n",
            "it 2981/5000, loss = 0.288594\n",
            "it 2991/5000, loss = 0.442508\n",
            "it 3001/5000, loss = 0.409638\n",
            "it 3011/5000, loss = 0.360268\n",
            "it 3021/5000, loss = 0.387662\n",
            "it 3031/5000, loss = 0.393844\n",
            "it 3041/5000, loss = 0.495821\n",
            "it 3051/5000, loss = 0.441319\n",
            "it 3061/5000, loss = 0.319430\n",
            "it 3071/5000, loss = 0.447529\n",
            "it 3081/5000, loss = 0.387324\n",
            "it 3091/5000, loss = 0.416670\n",
            "it 3101/5000, loss = 0.367711\n",
            "it 3111/5000, loss = 0.440897\n",
            "it 3121/5000, loss = 0.384463\n",
            "it 3131/5000, loss = 0.316084\n",
            "it 3141/5000, loss = 0.485595\n",
            "it 3151/5000, loss = 0.437639\n",
            "it 3161/5000, loss = 0.568932\n",
            "it 3171/5000, loss = 0.595722\n",
            "it 3181/5000, loss = 0.502766\n",
            "it 3191/5000, loss = 0.425161\n",
            "it 3201/5000, loss = 0.478155\n",
            "it 3211/5000, loss = 0.505285\n",
            "it 3221/5000, loss = 0.556884\n",
            "it 3231/5000, loss = 0.400887\n",
            "it 3241/5000, loss = 0.470606\n",
            "it 3251/5000, loss = 0.531686\n",
            "it 3261/5000, loss = 0.506177\n",
            "it 3271/5000, loss = 0.516563\n",
            "it 3281/5000, loss = 0.394795\n",
            "it 3291/5000, loss = 0.626278\n",
            "it 3301/5000, loss = 0.327060\n",
            "it 3311/5000, loss = 0.459460\n",
            "it 3321/5000, loss = 0.425911\n",
            "it 3331/5000, loss = 0.364971\n",
            "it 3341/5000, loss = 0.473767\n",
            "it 3351/5000, loss = 0.574486\n",
            "it 3361/5000, loss = 0.448685\n",
            "it 3371/5000, loss = 0.437180\n",
            "it 3381/5000, loss = 0.451316\n",
            "it 3391/5000, loss = 0.363466\n",
            "it 3401/5000, loss = 0.370122\n",
            "it 3411/5000, loss = 0.458126\n",
            "it 3421/5000, loss = 0.360440\n",
            "it 3431/5000, loss = 0.549224\n",
            "it 3441/5000, loss = 0.517484\n",
            "it 3451/5000, loss = 0.440438\n",
            "it 3461/5000, loss = 0.500539\n",
            "it 3471/5000, loss = 0.354493\n",
            "it 3481/5000, loss = 0.485905\n",
            "it 3491/5000, loss = 0.545709\n",
            "it 3501/5000, loss = 0.492333\n",
            "it 3511/5000, loss = 0.457380\n",
            "it 3521/5000, loss = 0.348798\n",
            "it 3531/5000, loss = 0.385476\n",
            "it 3541/5000, loss = 0.563568\n",
            "it 3551/5000, loss = 0.428875\n",
            "it 3561/5000, loss = 0.480173\n",
            "it 3571/5000, loss = 0.378135\n",
            "it 3581/5000, loss = 0.434993\n",
            "it 3591/5000, loss = 0.529648\n",
            "it 3601/5000, loss = 0.610856\n",
            "it 3611/5000, loss = 0.475278\n",
            "it 3621/5000, loss = 0.542830\n",
            "it 3631/5000, loss = 0.455161\n",
            "it 3641/5000, loss = 0.466036\n",
            "it 3651/5000, loss = 0.443997\n",
            "it 3661/5000, loss = 0.607541\n",
            "it 3671/5000, loss = 0.469059\n",
            "it 3681/5000, loss = 0.578203\n",
            "it 3691/5000, loss = 0.422471\n",
            "it 3701/5000, loss = 0.464361\n",
            "it 3711/5000, loss = 0.496485\n",
            "it 3721/5000, loss = 0.608674\n",
            "it 3731/5000, loss = 0.277578\n",
            "it 3741/5000, loss = 0.294410\n",
            "it 3751/5000, loss = 0.305866\n",
            "it 3761/5000, loss = 0.377933\n",
            "it 3771/5000, loss = 0.490201\n",
            "it 3781/5000, loss = 0.437015\n",
            "it 3791/5000, loss = 0.434461\n",
            "it 3801/5000, loss = 0.324114\n",
            "it 3811/5000, loss = 0.547031\n",
            "it 3821/5000, loss = 0.356571\n",
            "it 3831/5000, loss = 0.486576\n",
            "it 3841/5000, loss = 0.438238\n",
            "it 3851/5000, loss = 0.454508\n",
            "it 3861/5000, loss = 0.448553\n",
            "it 3871/5000, loss = 0.515612\n",
            "it 3881/5000, loss = 0.486854\n",
            "it 3891/5000, loss = 0.393976\n",
            "it 3901/5000, loss = 0.377454\n",
            "it 3911/5000, loss = 0.383107\n",
            "it 3921/5000, loss = 0.428415\n",
            "it 3931/5000, loss = 0.614481\n",
            "it 3941/5000, loss = 0.384503\n",
            "it 3951/5000, loss = 0.427009\n",
            "it 3961/5000, loss = 0.430056\n",
            "it 3971/5000, loss = 0.452929\n",
            "it 3981/5000, loss = 0.371304\n",
            "it 3991/5000, loss = 0.490155\n",
            "it 4001/5000, loss = 0.337309\n",
            "it 4011/5000, loss = 0.411987\n",
            "it 4021/5000, loss = 0.474051\n",
            "it 4031/5000, loss = 0.533039\n",
            "it 4041/5000, loss = 0.512228\n",
            "it 4051/5000, loss = 0.435011\n",
            "it 4061/5000, loss = 0.374006\n",
            "it 4071/5000, loss = 0.353162\n",
            "it 4081/5000, loss = 0.541770\n",
            "it 4091/5000, loss = 0.363803\n",
            "it 4101/5000, loss = 0.489645\n",
            "it 4111/5000, loss = 0.384272\n",
            "it 4121/5000, loss = 0.482824\n",
            "it 4131/5000, loss = 0.594461\n",
            "it 4141/5000, loss = 0.331662\n",
            "it 4151/5000, loss = 0.344756\n",
            "it 4161/5000, loss = 0.377249\n",
            "it 4171/5000, loss = 0.364470\n",
            "it 4181/5000, loss = 0.376696\n",
            "it 4191/5000, loss = 0.401384\n",
            "it 4201/5000, loss = 0.360179\n",
            "it 4211/5000, loss = 0.443434\n",
            "it 4221/5000, loss = 0.507824\n",
            "it 4231/5000, loss = 0.426559\n",
            "it 4241/5000, loss = 0.347501\n",
            "it 4251/5000, loss = 0.422523\n",
            "it 4261/5000, loss = 0.508738\n",
            "it 4271/5000, loss = 0.357140\n",
            "it 4281/5000, loss = 0.359319\n",
            "it 4291/5000, loss = 0.408503\n",
            "it 4301/5000, loss = 0.529037\n",
            "it 4311/5000, loss = 0.435943\n",
            "it 4321/5000, loss = 0.457094\n",
            "it 4331/5000, loss = 0.516684\n",
            "it 4341/5000, loss = 0.364198\n",
            "it 4351/5000, loss = 0.339987\n",
            "it 4361/5000, loss = 0.427578\n",
            "it 4371/5000, loss = 0.468513\n",
            "it 4381/5000, loss = 0.515113\n",
            "it 4391/5000, loss = 0.430024\n",
            "it 4401/5000, loss = 0.319379\n",
            "it 4411/5000, loss = 0.457157\n",
            "it 4421/5000, loss = 0.347613\n",
            "it 4431/5000, loss = 0.522907\n",
            "it 4441/5000, loss = 0.455648\n",
            "it 4451/5000, loss = 0.355990\n",
            "it 4461/5000, loss = 0.465330\n",
            "it 4471/5000, loss = 0.591457\n",
            "it 4481/5000, loss = 0.529469\n",
            "it 4491/5000, loss = 0.378045\n",
            "it 4501/5000, loss = 0.391701\n",
            "it 4511/5000, loss = 0.621861\n",
            "it 4521/5000, loss = 0.457297\n",
            "it 4531/5000, loss = 0.371801\n",
            "it 4541/5000, loss = 0.492634\n",
            "it 4551/5000, loss = 0.378438\n",
            "it 4561/5000, loss = 0.563164\n",
            "it 4571/5000, loss = 0.418497\n",
            "it 4581/5000, loss = 0.555779\n",
            "it 4591/5000, loss = 0.395609\n",
            "it 4601/5000, loss = 0.546288\n",
            "it 4611/5000, loss = 0.433206\n",
            "it 4621/5000, loss = 0.339802\n",
            "it 4631/5000, loss = 0.518616\n",
            "it 4641/5000, loss = 0.662588\n",
            "it 4651/5000, loss = 0.533813\n",
            "it 4661/5000, loss = 0.391621\n",
            "it 4671/5000, loss = 0.381957\n",
            "it 4681/5000, loss = 0.502038\n",
            "it 4691/5000, loss = 0.611102\n",
            "it 4701/5000, loss = 0.470551\n",
            "it 4711/5000, loss = 0.367915\n",
            "it 4721/5000, loss = 0.492634\n",
            "it 4731/5000, loss = 0.492774\n",
            "it 4741/5000, loss = 0.609958\n",
            "it 4751/5000, loss = 0.475200\n",
            "it 4761/5000, loss = 0.436788\n",
            "it 4771/5000, loss = 0.530562\n",
            "it 4781/5000, loss = 0.392550\n",
            "it 4791/5000, loss = 0.566593\n",
            "it 4801/5000, loss = 0.474210\n",
            "it 4811/5000, loss = 0.484644\n",
            "it 4821/5000, loss = 0.357377\n",
            "it 4831/5000, loss = 0.439465\n",
            "it 4841/5000, loss = 0.334024\n",
            "it 4851/5000, loss = 0.424775\n",
            "it 4861/5000, loss = 0.502680\n",
            "it 4871/5000, loss = 0.358945\n",
            "it 4881/5000, loss = 0.429743\n",
            "it 4891/5000, loss = 0.553868\n",
            "it 4901/5000, loss = 0.599561\n",
            "it 4911/5000, loss = 0.367451\n",
            "it 4921/5000, loss = 0.377477\n",
            "it 4931/5000, loss = 0.486706\n",
            "it 4941/5000, loss = 0.298846\n",
            "it 4951/5000, loss = 0.477577\n",
            "it 4961/5000, loss = 0.488942\n",
            "it 4971/5000, loss = 0.420974\n",
            "it 4981/5000, loss = 0.386047\n",
            "it 4991/5000, loss = 0.575764\n",
            "score for train: 0.9095735607675907\n",
            "score for test: 0.9006060606060606\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CfJYIJ81Bu1j"
      },
      "execution_count": 34,
      "outputs": []
    }
  ]
}