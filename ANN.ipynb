{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNMPKEoNv5+JUV99jo1+5kf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/datle2403/datle2403/blob/main/ANN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "xESsUkIKNwPI"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import fetch_openml\n",
        "from sklearn.model_selection import train_test_split\n",
        "X, y= fetch_openml('mnist_784', version=1, return_X_y= True)\n",
        "y=y.astype(int)\n",
        "X= ((X/255.)-0.5)* 2\n",
        "X_train, X_test, y_train, y_test= train_test_split(X,y ,test_size=10000, random_state=123, stratify=y)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "mnist=np.load('mnist_scaled.npz')"
      ],
      "metadata": {
        "id": "Yw0_mTnwf503"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, y_train, X_test, y_test = [mnist[f] for f in ['X_train', 'y_train', \n",
        "                                    'X_test', 'y_test']]"
      ],
      "metadata": {
        "id": "aExBc6_Rfu2u"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aD7xhpMfgHTO",
        "outputId": "e8785de3-a948-4f0d-89b4-e93b11bb3cea"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(60000, 784)"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "import numpy as np\n",
        "class NeuralNetMLP(object):\n",
        "  \"\"\"\n",
        "  Parameter:\n",
        "  n_hidden= number of hidden unit\n",
        "  l2: lambda value for L2-regularization\n",
        "  epochs: number of passes over the training set\n",
        "  eta: learning rate\n",
        "  shuffle: shuffle training set every epoch\n",
        "  minibatch_size: number of training examples per minibatch\n",
        "  seed: random seed for initializing weights and shuffling\n",
        "\n",
        "  Attributes\n",
        "  eval_: dict collecting cost, training accuracy, valid accuracy for each epoch\n",
        "  \"\"\"\n",
        "  def __init__(self, n_hidden=30, l2=0, epochs= 100, eta=0.01, shuffle=True, minibatch_size=1, seed=None):\n",
        "    self.random= np.random.RandomState(seed)\n",
        "    self.n_hidden= n_hidden\n",
        "    self.l2=l2\n",
        "    self.eta=eta\n",
        "    self.epochs=epochs\n",
        "    self.shuffle=shuffle\n",
        "    self.minibatch_size=minibatch_size\n",
        "  def _onehot(self, y, n_classes):\n",
        "    \"\"\"\n",
        "    Encode label into one-hot\n",
        "\n",
        "    y: shape=[n_examples]\n",
        "    return onehot: shape= (n_examples, n_labels)\n",
        "    \"\"\"\n",
        "    onehot= np.zeros((n_classes, y.shape[0]))\n",
        "    for idx, val in enumerate(y.astype(int)):\n",
        "      onehot[val,idx]=1.\n",
        "    return onehot.T\n",
        "  def _sigmoid(self, z):\n",
        "    return 1./(1.+ np.exp(-np.clip(z,-250,250)))\n",
        "  def _forward(self, X):\n",
        "        \"\"\"Compute forward propagation step\"\"\"\n",
        "\n",
        "        # step 1: net input of hidden layer\n",
        "        # [n_examples, n_features] dot [n_features, n_hidden]\n",
        "        # -> [n_examples, n_hidden]\n",
        "        z_h = np.dot(X, self.w_h) + self.b_h\n",
        "\n",
        "        # step 2: activation of hidden layer\n",
        "        a_h = self._sigmoid(z_h)\n",
        "\n",
        "        # step 3: net input of output layer\n",
        "        # [n_examples, n_hidden] dot [n_hidden, n_classlabels]\n",
        "        # -> [n_examples, n_classlabels]\n",
        "\n",
        "        z_out = np.dot(a_h, self.w_out) + self.b_out\n",
        "\n",
        "        # step 4: activation output layer\n",
        "        a_out = self._sigmoid(z_out)\n",
        "\n",
        "        return z_h, a_h, z_out, a_out\n",
        "\n",
        "  def _compute_cost(self, y_enc, output):\n",
        "    \"\"\"\n",
        "    Compute the cost function\n",
        "    y_enc: one_hot encoded class labels\n",
        "    output: activation of output layer\n",
        "    \"\"\"\n",
        "    L2_term= (self.l2 * (np.sum(self.w_h ** 2.) + np.sum(self.w_out ** 2.)))\n",
        "    term1= -y_enc * (np.log(output))\n",
        "    term2= (1.- y_enc) * np.log(1. -output)\n",
        "    cost= np.sum(term1 - term2)+ L2_term\n",
        "    return cost\n",
        "  def predict(self, X):\n",
        "    z_h, a_h, z_out, a_out= self._forward(X)\n",
        "    y_pred= np.argmax(z_out, axis=1)\n",
        "    return y_pred\n",
        "  def fit(self, X_train, y_train, X_valid, y_valid):\n",
        "    n_output= np.unique(y_train).shape[0]\n",
        "    n_features= X_train.shape[1]\n",
        "\n",
        "    self.b_h= np.zeros(self.n_hidden)\n",
        "    self.w_h= self.random.normal(loc=0.0, scale=0.1, size=(n_features, self.n_hidden))\n",
        "\n",
        "    self.b_out=np.zeros(n_output)\n",
        "    self.w_out=self.random.normal(loc=0.0, scale=0.1, size=(self.n_hidden, n_output))\n",
        "\n",
        "    epoch_strlen= len(str(self.epochs))\n",
        "    self.eval_= {'cost': [], 'train_acc':[], 'valid_acc': []}\n",
        "\n",
        "    y_train_enc= self._onehot(y_train, n_output)\n",
        "    \n",
        "    #iterate over training epochs\n",
        "\n",
        "    for i in range(self.epochs):\n",
        "\n",
        "      # iterate over minibatchs\n",
        "      indices= np.arange(X_train.shape[0])\n",
        "      if self.shuffle:\n",
        "        self.random.shuffle(indices)\n",
        "      for start_idx in range(0, indices.shape[0] - self.minibatch_size +\n",
        "                                   1, self.minibatch_size):\n",
        "        batch_idx = indices[start_idx:start_idx + self.minibatch_size]\n",
        "        # forward prop\n",
        "        z_h, a_h, z_out, a_out= self._forward(X_train[batch_idx])\n",
        "\n",
        "        # back_ prop\n",
        "        delta_out = a_out - y_train_enc[batch_idx]\n",
        "\n",
        "        sigmoid_prime_h= a_h * (1. - a_h)\n",
        "\n",
        "        delta_h= (np.dot(delta_out, self.w_out.T) * sigmoid_prime_h)\n",
        "\n",
        "        grad_w_h= np.dot(X_train[batch_idx].T, delta_h)\n",
        "        grad_b_h= np.sum(delta_h, axis=0)\n",
        "\n",
        "        grad_w_out= np.dot(a_h.T, delta_out)\n",
        "        grad_b_out= np.sum(delta_out, axis=0)\n",
        "\n",
        "        #regularization and weight update\n",
        "        delta_w_h= (grad_w_h+ self.l2 * self.w_h)\n",
        "        delta_b_h= grad_b_h\n",
        "        self.w_h -=self.eta * delta_w_h\n",
        "        self.b_h -=self.eta * delta_b_h\n",
        "        \n",
        "        delta_w_out= (grad_w_out + self.l2* self.w_out)\n",
        "        delta_b_out= grad_b_out\n",
        "        self.w_out-= self.eta * delta_w_out \n",
        "        self.b_out-= self.eta * delta_b_out\n",
        "\n",
        "      # evaluate for each epochs\n",
        "      z_h, a_h, z_out, a_out= self._forward(X_train)\n",
        "      cost= self._compute_cost(y_enc=y_train_enc, output=a_out)\n",
        "      y_train_pred= self.predict(X_train)\n",
        "      y_valid_pred= self.predict(X_valid)\n",
        "\n",
        "      train_acc= ((np.sum(y_train == y_train_pred)).astype(np.float)/X_train.shape[0])\n",
        "      valid_acc= ((np.sum(y_valid == y_valid_pred)).astype(np.float)/X_valid.shape[0])\n",
        "\n",
        "      sys.stderr.write('\\r%0*d/%d | Cost: %.2f '\n",
        "                             '| Train/Valid Acc.: %.2f%%/%.2f%% ' %\n",
        "                             (epoch_strlen, i+1, self.epochs, cost,\n",
        "                              train_acc*100, valid_acc*100))\n",
        "      sys.stderr.flush()\n",
        "\n",
        "      self.eval_['cost'].append(cost)\n",
        "      self.eval_['train_acc'].append(train_acc)\n",
        "      self.eval_['valid_acc'].append(valid_acc)\n",
        "    return self\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "-tgUS8sQOlAU"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "nn = NeuralNetMLP(n_hidden=100, \n",
        "                  l2=0.01, \n",
        "                  epochs=200, \n",
        "                  eta=0.0005,\n",
        "                  minibatch_size=100, \n",
        "                  shuffle=True,\n",
        "                  seed=1)\n",
        "nn.fit(X_train=X_train[:55000], \n",
        "       y_train=y_train[:55000],\n",
        "       X_valid=X_train[55000:],\n",
        "       y_valid=y_train[55000:])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yLseOn9QinT9",
        "outputId": "417a3b7e-37cb-4c7e-a2d6-8bb4cb67c9f6"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-16-c9c7d620cda3>:134: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  train_acc= ((np.sum(y_train == y_train_pred)).astype(np.float)/X_train.shape[0])\n",
            "<ipython-input-16-c9c7d620cda3>:135: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  valid_acc= ((np.sum(y_valid == y_valid_pred)).astype(np.float)/X_valid.shape[0])\n",
            "200/200 | Cost: 5065.78 | Train/Valid Acc.: 99.28%/97.98% "
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<__main__.NeuralNetMLP at 0x7f55e92ef580>"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "m[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3TkXWG1BRtxA",
        "outputId": "9b8e0d75-a664-4c73-9633-ffa32e7c511a"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
              "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
              "       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
              "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.arange(10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rfwtCsTMRymD",
        "outputId": "22ef3187-1388-4826-ca20-e868161af77c"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "r2NXACrTWEia"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}